# LAB8_Haberman.py

# ----------------------------
# Import ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (‡∏ï‡∏≤‡∏°‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô)
# ----------------------------
import numpy as np  # (‡∏ö‡∏ó 2 ‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤) ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå
import pandas as pd  # (‡∏ö‡∏ó 1 ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ dataset ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á
import tensorflow as tf  # (‡∏ö‡∏ó 10 Neural Network) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å ANN
from sklearn.model_selection import train_test_split, cross_val_score  # (‡∏ö‡∏ó 7 ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö train/test ‡πÅ‡∏•‡∏∞ Cross-Validation
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # (‡∏ö‡∏ó 1) ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ Label
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score  # (‡∏ö‡∏ó 7) ‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
from sklearn.naive_bayes import GaussianNB  # (‡∏ö‡∏ó 5 Naive Bayes) ‡∏ï‡∏±‡∏ß‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô
from sklearn.tree import DecisionTreeClassifier  # (‡∏ö‡∏ó 4 Decision Tree) ‡∏ï‡∏±‡∏ß‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ
from sklearn.neighbors import KNeighborsClassifier  # (‡∏ö‡∏ó 3 & 6 KNN) ‡∏ï‡∏±‡∏ß‡∏à‡∏≥‡πÅ‡∏ô‡∏Å K-Nearest Neighbors
from sklearn.cluster import KMeans  # (‡∏ö‡∏ó 9 Clustering) ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• K-Means
from sklearn.feature_selection import SelectKBest, f_classif  # (‡∏ö‡∏ó 8 Feature Selection) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

# ----------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢ (‡∏ö‡∏ó 7 ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•)
# ----------------------------
def compute_metrics(y_true, y_pred):  
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î Accuracy, Precision, Recall, F1
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average="macro")
    rec = recall_score(y_true, y_pred, average="macro")
    f1 = f1_score(y_true, y_pred, average="macro")
    return {"Accuracy": acc, "Precision": prec, "Recall": rec, "F1 Score": f1}

def print_metrics_html(method_name, metrics):  
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á HTML
    df = pd.DataFrame([metrics], index=[method_name])
    return df.to_html(float_format="%.4f", classes="table table-striped table-bordered table-hover", justify="center", border=1)

# ----------------------------
# ‡πÇ‡∏´‡∏•‡∏î Dataset Haberman (‡∏ö‡∏ó 1 ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
# ----------------------------
data = pd.read_csv("haberman.csv", header=None)  # ‡πÇ‡∏´‡∏•‡∏î CSV
X = data.iloc[:, :-1].values  # Features (age, year, nodes)
y = data.iloc[:, -1].values   # Label (1 = ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏≠‡∏î, 2 = ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏î)

# ‡πÅ‡∏ö‡πà‡∏á train/test 70/30
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)

# Scaling ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Normalize)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# One-Hot Encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ANN (‡∏ö‡∏ó 10)
encoder = OneHotEncoder(sparse_output=False)
y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))
y_test_onehot = encoder.transform(y_test.reshape(-1, 1))

# ----------------------------
# 1‚Äì2. Representation & Problem Solving (‡∏ö‡∏ó 2)
# ----------------------------
def create_adjacency_matrix(X, k=3):  
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á adjacency matrix ‡πÅ‡∏ó‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô
    n_samples = X.shape[0]
    adj_matrix = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        distances = np.sum((X - X[i])**2, axis=1)  # ‡∏£‡∏∞‡∏¢‡∏∞ Euclidean
        nearest_indices = np.argsort(distances)[1:k+1]  # ‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡πÉ‡∏Å‡∏•‡πâ‡∏™‡∏∏‡∏î k ‡∏ï‡∏±‡∏ß
        adj_matrix[i, nearest_indices] = 1
    return adj_matrix

adj_matrix = create_adjacency_matrix(X_train_scaled)
adj_html = "<h3>‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Adjacency Matrix)</h3>"

# ----------------------------
# 8. Feature Selection (‡∏ö‡∏ó 8)
# ----------------------------
selector = SelectKBest(score_func=f_classif, k=2)  
X_train_selected = selector.fit_transform(X_train_scaled, y_train)
X_test_selected = selector.transform(X_test_scaled)

# ----------------------------
# 5. Naive Bayes (‡∏ö‡∏ó 5)
# ----------------------------
nb = GaussianNB()
nb.fit(X_train_selected, y_train)
nb_pred = nb.predict(X_test_selected)
nb_metrics = compute_metrics(y_test, nb_pred)

# ----------------------------
# 4. Decision Tree (‡∏ö‡∏ó 4)
# ----------------------------
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train_selected, y_train)
dt_pred = dt.predict(X_test_selected)
dt_metrics = compute_metrics(y_test, dt_pred)

# ----------------------------
# 3 & 6. KNN (‡∏ö‡∏ó 3 + 6)
# ----------------------------
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_selected, y_train)
knn_pred = knn.predict(X_test_selected)
knn_metrics = compute_metrics(y_test, knn_pred)

# ----------------------------
# 9. K-Means Clustering (‡∏ö‡∏ó 9)
# ----------------------------
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X_train_selected)
cluster_labels = kmeans.predict(X_test_selected)
cluster_score = adjusted_rand_score(y_test, cluster_labels)
cluster_html = f"<h3>Clustering (K-Means)</h3><p>Adjusted Rand Score: {cluster_score:.4f}</p>"

# ----------------------------
# 10. ANN (‡∏ö‡∏ó 10 Neural Network)
# ----------------------------
ann = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(2,)),  # input 2 features
    tf.keras.layers.Dense(10, activation="relu"),  # hidden layer
    tf.keras.layers.Dense(2, activation="softmax")  # output layer
])
ann.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
ann.fit(X_train_selected, y_train_onehot, epochs=100, batch_size=5, verbose=0)

ann_pred_onehot = ann.predict(X_test_selected)
ann_pred = np.argmax(ann_pred_onehot, axis=1) + 1  # ‡∏ö‡∏ß‡∏Å 1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ label ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö dataset
ann_metrics = compute_metrics(y_test, ann_pred)

# ----------------------------
# 7. Cross Validation (‡∏ö‡∏ó 7)
# ----------------------------
cv_scores = cross_val_score(DecisionTreeClassifier(random_state=42), X_train_selected, y_train, cv=5)
cv_html = f"<h3>‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (Cross-Validation ‡∏ö‡∏ô Decision Tree)</h3><p>Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})</p>"

# ----------------------------
# HTML Output (‡∏ò‡∏µ‡∏°‡∏ó‡πâ‡∏≠‡∏á‡∏ü‡πâ‡∏≤‡∏¢‡∏≤‡∏°‡∏Ñ‡πà‡∏≥‡∏Ñ‡∏∑‡∏ô)
# ----------------------------
html_output = """
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ AI (Haberman Dataset)</title>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
<style>
  body {
      padding: 20px;
      background: linear-gradient(to bottom, #0f2027, #203a43, #2c5364); /* ‡∏ó‡πâ‡∏≠‡∏á‡∏ü‡πâ‡∏≤‡∏¢‡∏≤‡∏°‡∏Ñ‡πà‡∏≥‡∏Ñ‡∏∑‡∏ô */
      color: #f1f1f1;
      font-family: "Segoe UI", sans-serif;
  }
  h1, h2, h3 {
      color: #ffdd57; /* ‡∏™‡∏µ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏ó‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ */
      text-shadow: 2px 2px 5px rgba(0,0,0,0.8);
  }
  .table {
      margin: auto;
      width: 85%;
      background: rgba(255,255,255,0.05);
      color: #fff;
  }
  .table th {
      background-color: rgba(0, 0, 50, 0.7);
      color: #ffdd57;
  }
  .table td {
      background-color: rgba(255, 255, 255, 0.05);
  }
  .container {
      background: rgba(0,0,0,0.4);
      padding: 20px;
      border-radius: 15px;
      box-shadow: 0 0 30px rgba(0,0,0,0.7);
  }
</style>
</head>
<body>
<div class="container">
<h1 class="text-center">üåå ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ AI (Haberman Dataset) üåå</h1>
<h2>üîÆ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å</h2>
"""
html_output += print_metrics_html("Naive Bayes", nb_metrics)
html_output += print_metrics_html("Decision Tree", dt_metrics)
html_output += print_metrics_html("KNN", knn_metrics)
html_output += print_metrics_html("ANN", ann_metrics)

html_output += adj_html
html_output += cluster_html
html_output += cv_html
html_output += "</div></body></html>"

with open("haberman_results.html", "w", encoding="utf-8") as f:
    f.write(html_output)

print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå HTML ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'haberman_results.html' ‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏π‡πÉ‡∏ô‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢")
