# LAB8_Haberman.py

# ----------------------------
# Import ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (‡∏ï‡∏≤‡∏°‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô)
# ----------------------------
import numpy as np  # (‡∏ö‡∏ó 2 ‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤) ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç, ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå, ‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå
import pandas as pd  # (‡∏ö‡∏ó 1 ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ dataset, ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á
import tensorflow as tf  # (‡∏ö‡∏ó 10 Neural Network) ‡πÉ‡∏ä‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å Artificial Neural Network (ANN)
from sklearn.model_selection import train_test_split, cross_val_score  # (‡∏ö‡∏ó 7 ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•) ‡πÅ‡∏ö‡πà‡∏á train/test ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ Cross-Validation
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # (‡∏ö‡∏ó 1) StandardScaler ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ feature ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô, OneHot ‡πÅ‡∏õ‡∏•‡∏á label
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score  # (‡∏ö‡∏ó 7) ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏±‡∏î‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•
from sklearn.naive_bayes import GaussianNB  # (‡∏ö‡∏ó 5 Naive Bayes) ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô
from sklearn.tree import DecisionTreeClassifier  # (‡∏ö‡∏ó 4 Decision Tree) ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à
from sklearn.neighbors import KNeighborsClassifier  # (‡∏ö‡∏ó 3 + 6 KNN) ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
from sklearn.cluster import KMeans  # (‡∏ö‡∏ó 9 Clustering) ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° K-Means
from sklearn.feature_selection import SelectKBest, f_classif  # (‡∏ö‡∏ó 8 Feature Selection) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å feature ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏î‡πâ‡∏ß‡∏¢ ANOVA F-test

# ----------------------------
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢ (‡∏ö‡∏ó 7 ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•)
# ----------------------------
def compute_metrics(y_true, y_pred):  
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á (y_true) ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (y_pred)
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î 4 ‡∏ï‡∏±‡∏ß: Accuracy, Precision, Recall, F1
    acc = accuracy_score(y_true, y_pred)  # Accuracy = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å / ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    prec = precision_score(y_true, y_pred, average="macro")  # Precision = ‡∏ó‡∏≤‡∏¢‡∏ß‡πà‡∏≤ "‡πÉ‡∏ä‡πà" ‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡∏π‡∏Å‡∏Å‡∏µ‡πà %
    rec = recall_score(y_true, y_pred, average="macro")  # Recall = ‡∏Ç‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≤‡∏¢‡πÄ‡∏à‡∏≠‡∏Ñ‡∏£‡∏ö‡∏Å‡∏µ‡πà %
    f1 = f1_score(y_true, y_pred, average="macro")  # F1 = ‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á Precision + Recall
    return {"Accuracy": acc, "Precision": prec, "Recall": rec, "F1 Score": f1}  # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô dictionary

def print_metrics_html(method_name, metrics):  
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ò‡∏µ (method_name) + ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metric
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame (‡∏ï‡∏≤‡∏£‡∏≤‡∏á) ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô HTML ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏ä‡∏ß‡πå‡∏ö‡∏ô‡πÄ‡∏ß‡πá‡∏ö
    df = pd.DataFrame([metrics], index=[method_name])  
    return df.to_html(float_format="%.4f", classes="table table-striped table-bordered table-hover", justify="center", border=1)

# ----------------------------
# ‡πÇ‡∏´‡∏•‡∏î Dataset Haberman (‡∏ö‡∏ó 1 ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
# ----------------------------
data = pd.read_csv("haberman.csv", header=None)  # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå haberman.csv (‡πÑ‡∏°‡πà‡∏°‡∏µ header)
X = data.iloc[:, :-1].values  # Features = ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏£‡∏Å‡∏ñ‡∏∂‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (3 ‡∏ï‡∏±‡∏ß: ‡∏≠‡∏≤‡∏¢‡∏∏, ‡∏õ‡∏µ‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡∏°‡∏ô‡πâ‡∏≥‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á)
y = data.iloc[:, -1].values   # Label = ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (1 = ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏≠‡∏î, 2 = ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏î)

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/test ‚Üí 70% train, 30% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)  # stratify=y ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ class 1/2 ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á train/test

# Scaling ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Normalize)
scaler = StandardScaler()  # StandardScaler: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏°‡∏µ mean=0, std=1
X_train_scaled = scaler.fit_transform(X_train)  # fit+transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å
X_test_scaled = scaler.transform(X_test)  # ‡πÉ‡∏ä‡πâ scaler ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏õ‡∏•‡∏á test

# One-Hot Encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ANN (‡∏ö‡∏ó 10 Neural Network)
encoder = OneHotEncoder(sparse_output=False)  # OneHot = ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô class 1,2 ‚Üí [1,0], [0,1]
y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))  # ‡πÅ‡∏õ‡∏•‡∏á y_train
y_test_onehot = encoder.transform(y_test.reshape(-1, 1))  # ‡πÅ‡∏õ‡∏•‡∏á y_test

# ----------------------------
# 1‚Äì2. Representation & Problem Solving (‡∏ö‡∏ó 2)
# ----------------------------
def create_adjacency_matrix(X, k=3):  
    # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á Adjacency Matrix (‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
    n_samples = X.shape[0]  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÉ‡∏ô X = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô sample
    adj_matrix = np.zeros((n_samples, n_samples))  # ‡∏™‡∏£‡πâ‡∏≤‡∏á matrix ‡∏®‡∏π‡∏ô‡∏¢‡πå (n x n)
    for i in range(n_samples):  # loop ‡∏ó‡∏∏‡∏Å sample
        distances = np.sum((X - X[i])**2, axis=1)  # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞ Euclidean (square) ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á sample i ‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß
        nearest_indices = np.argsort(distances)[1:k+1]  # ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ index ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î k ‡∏ï‡∏±‡∏ß (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á)
        adj_matrix[i, nearest_indices] = 1  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ 1 ‡∏ó‡∏µ‡πà‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô
    return adj_matrix  # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ adjacency matrix

adj_matrix = create_adjacency_matrix(X_train_scaled)  # ‡∏™‡∏£‡πâ‡∏≤‡∏á adjacency matrix ‡∏Ç‡∏≠‡∏á training set
adj_html = "<h3>‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Adjacency Matrix)</h3>"  # HTML ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ä‡∏ß‡πå‡∏ú‡∏•

# ----------------------------
# 8. Feature Selection (‡∏ö‡∏ó 8)
# ----------------------------
selector = SelectKBest(score_func=f_classif, k=2)  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å feature ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 2 ‡∏ï‡∏±‡∏ß ‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 3
X_train_selected = selector.fit_transform(X_train_scaled, y_train)  # fit+transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å
X_test_selected = selector.transform(X_test_scaled)  # transform ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö

# ----------------------------
# 5. Naive Bayes (‡∏ö‡∏ó 5)
# ----------------------------
nb = GaussianNB()  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Naive Bayes
nb.fit(X_train_selected, y_train)  # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train
nb_pred = nb.predict(X_test_selected)  # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ test set
nb_metrics = compute_metrics(y_test, nb_pred)  # ‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ Accuracy, Precision, Recall, F1

# ----------------------------
# 4. Decision Tree (‡∏ö‡∏ó 4)
# ----------------------------
dt = DecisionTreeClassifier(random_state=42)  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Decision Tree
dt.fit(X_train_selected, y_train)  # ‡∏ù‡∏∂‡∏Å
dt_pred = dt.predict(X_test_selected)  # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
dt_metrics = compute_metrics(y_test, dt_pred)  # ‡∏ß‡∏±‡∏î‡∏ú‡∏•

# ----------------------------
# 3 & 6. KNN (‡∏ö‡∏ó 3 + 6)
# ----------------------------
knn = KNeighborsClassifier(n_neighbors=5)  # ‡πÉ‡∏ä‡πâ K=5 (‡∏´‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡∏™‡∏∏‡∏î 5 ‡∏ï‡∏±‡∏ß)
knn.fit(X_train_selected, y_train)  # ‡∏ù‡∏∂‡∏Å
knn_pred = knn.predict(X_test_selected)  # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
knn_metrics = compute_metrics(y_test, knn_pred)  # ‡∏ß‡∏±‡∏î‡∏ú‡∏•

# ----------------------------
# 9. K-Means Clustering (‡∏ö‡∏ó 9)
# ----------------------------
kmeans = KMeans(n_clusters=2, random_state=42)  # ‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 2 cluster
kmeans.fit(X_train_selected)  # ‡∏ù‡∏∂‡∏Å KMeans ‡∏î‡πâ‡∏ß‡∏¢ training set
cluster_labels = kmeans.predict(X_test_selected)  # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏° test set
cluster_score = adjusted_rand_score(y_test, cluster_labels)  # ‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏Å‡∏≤‡∏£ clustering ‡∏î‡πâ‡∏ß‡∏¢ ARI
cluster_html = f"<h3>Clustering (K-Means)</h3><p>Adjusted Rand Score: {cluster_score:.4f}</p>"

# ----------------------------
# 10. ANN (‡∏ö‡∏ó 10 Neural Network)
# ----------------------------
ann = tf.keras.Sequential([  # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö Sequential
    tf.keras.layers.Input(shape=(2,)),  # input = 2 feature (‡∏à‡∏≤‡∏Å Feature Selection)
    tf.keras.layers.Dense(10, activation="relu"),  # hidden layer ‡∏°‡∏µ 10 ‡∏ô‡∏¥‡∏ß‡∏£‡∏≠‡∏ô, ‡πÉ‡∏ä‡πâ ReLU
    tf.keras.layers.Dense(2, activation="softmax")  # output layer ‡∏°‡∏µ 2 ‡∏ô‡∏¥‡∏ß‡∏£‡∏≠‡∏ô (class 1/2), ‡πÉ‡∏ä‡πâ softmax
])
ann.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î loss + optimizer
ann.fit(X_train_selected, y_train_onehot, epochs=100, batch_size=5, verbose=0)  # ‡∏ù‡∏∂‡∏Å ANN 100 ‡∏£‡∏≠‡∏ö (batch = 5)

ann_pred_onehot = ann.predict(X_test_selected)  # ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (output = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô)
ann_pred = np.argmax(ann_pred_onehot, axis=1) + 1  # ‡πÅ‡∏õ‡∏•‡∏á one-hot ‚Üí class (‡∏ö‡∏ß‡∏Å 1 ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö label 1/2)
ann_metrics = compute_metrics(y_test, ann_pred)  # ‡∏ß‡∏±‡∏î‡∏ú‡∏•

# ----------------------------
# 7. Cross Validation (‡∏ö‡∏ó 7)
# ----------------------------
cv_scores = cross_val_score(DecisionTreeClassifier(random_state=42), X_train_selected, y_train, cv=5)  
# Cross-validation 5 fold (Decision Tree)
cv_html = f"<h3>‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (Cross-Validation ‡∏ö‡∏ô Decision Tree)</h3><p>Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})</p>"

# ----------------------------
# HTML Output (‡∏ò‡∏µ‡∏°‡∏ó‡πâ‡∏≠‡∏á‡∏ü‡πâ‡∏≤‡∏¢‡∏≤‡∏°‡∏Ñ‡πà‡∏≥‡∏Ñ‡∏∑‡∏ô)
# ----------------------------
html_output = """
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ AI (Haberman Dataset)</title>
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
<style>
  body {
      padding: 20px;
      background: linear-gradient(to bottom, #0f2027, #203a43, #2c5364); /* ‡∏ó‡πâ‡∏≠‡∏á‡∏ü‡πâ‡∏≤‡∏¢‡∏≤‡∏°‡∏Ñ‡πà‡∏≥‡∏Ñ‡∏∑‡∏ô */
      color: #f1f1f1;
      font-family: "Segoe UI", sans-serif;
  }
  h1, h2, h3 {
      color: #ffdd57; /* ‡∏™‡∏µ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡∏ó‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ */
      text-shadow: 2px 2px 5px rgba(0,0,0,0.8);
  }
  .table {
      margin: auto;
      width: 85%;
      background: rgba(255,255,255,0.05);
      color: #fff;
  }
  .table th {
      background-color: rgba(0, 0, 50, 0.7);
      color: #ffdd57;
  }
  .table td {
      background-color: rgba(255, 255, 255, 0.05);
  }
  .container {
      background: rgba(0,0,0,0.4);
      padding: 20px;
      border-radius: 15px;
      box-shadow: 0 0 30px rgba(0,0,0,0.7);
  }
</style>
</head>
<body>
<div class="container">
<h1 class="text-center">üåå ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ AI (Haberman Dataset) üåå</h1>
<h2>üîÆ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å</h2>
"""
html_output += print_metrics_html("Naive Bayes", nb_metrics)  # ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ú‡∏• Naive Bayes
html_output += print_metrics_html("Decision Tree", dt_metrics)  # ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ú‡∏• Decision Tree
html_output += print_metrics_html("KNN", knn_metrics)  # ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ú‡∏• KNN
html_output += print_metrics_html("ANN", ann_metrics)  # ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ú‡∏• ANN

html_output += adj_html  # ‡πÇ‡∏ä‡∏ß‡πå adjacency matrix
html_output += cluster_html  # ‡πÇ‡∏ä‡∏ß‡πå‡∏ú‡∏• K-Means
html_output += cv_html  # ‡πÇ‡∏ä‡∏ß‡πå‡∏ú‡∏• Cross-validation
html_output += "</div></body></html>"

with open("haberman_results.html", "w", encoding="utf-8") as f:  
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå HTML
    f.write(html_output)

print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå HTML ‡πÑ‡∏õ‡∏ó‡∏µ‡πà 'haberman_results.html' ‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏π‡πÉ‡∏ô‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢")
