import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV
df = pd.read_csv("Iris.csv")  # ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

# ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå Id ‡∏ó‡∏¥‡πâ‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Feature
df = df.drop(columns=["Id"])

# ‡πÅ‡∏¢‡∏Å Feature ‡πÅ‡∏•‡∏∞ Label
X = df.drop(columns=["Species"])
y = df["Species"]

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à
clf = DecisionTreeClassifier(criterion="entropy")
clf.fit(X_train, y_train)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ
plt.figure(figsize=(10, 6))
plot_tree(clf, filled=True, feature_names=X.columns, class_names=clf.classes_)
plt.title("üå≥ ‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Iris.csv")
plt.show()

# ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
accuracy = clf.score(X_test, y_test)
print(f"üéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•: {accuracy*100:.2f}%")

# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
sample = [[5.1, 3.5, 1.4, 0.2]]
prediction = clf.predict(sample)
print(f"\nüîÆ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á {sample} ‚Üí {prediction[0]}")

# ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏é‡πÅ‡∏ö‡∏ö if-then
rules = export_text(clf, feature_names=list(X.columns))
print("\nüìú ‡∏Å‡∏é‡∏à‡∏≤‡∏Å‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à:")
print(rules)
